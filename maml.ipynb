{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import grad\n",
    "from jax import vmap # for auto-vectorizing functions\n",
    "from functools import partial # for use with vmap\n",
    "from jax import jit # for compiling functions for speedup\n",
    "from jax import random # stax initialization uses jax.random\n",
    "from jax.experimental import stax # neural network library\n",
    "from jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax, Softplus # neural network layers\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "\n",
    "import numpy as onp\n",
    "from jax.experimental import optimizers\n",
    "from jax.tree_util import tree_multimap  # Element-wise manipulation of collections of numpy arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stax to set up network initialization and evaluation functions\n",
    "net_init, net_apply = stax.serial(\n",
    "    Dense(40), Relu,\n",
    "    Dense(40), Relu,\n",
    "    Dense(1)\n",
    ")\n",
    "\n",
    "rng = random.PRNGKey(0)\n",
    "in_shape = (-1, 1,)\n",
    "out_shape, net_params = net_init(rng, in_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stax to set up network initialization and evaluation functions\n",
    "net_init, net_apply = stax.serial(\n",
    "    Dense(40), Softplus,\n",
    "    Dense(40), Softplus,\n",
    "    Dense(1)\n",
    ")\n",
    "\n",
    "rng = random.PRNGKey(0)\n",
    "in_shape = (-1, 1,)\n",
    "out_shape, net_params = net_init(rng, in_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "\n",
    "def loss(params, inputs, targets):\n",
    "    # Computes average loss for the batch\n",
    "    predictions = net_apply(params, inputs)\n",
    "    return np.mean((targets - predictions)**2)\n",
    "\n",
    "def inner_update(p, x1, y1):\n",
    "    grads = grad(loss)(p, x1, y1)\n",
    "    inner_sgd_fn = lambda g, state: (state - alpha*g)\n",
    "    return tree_multimap(inner_sgd_fn, grads, p)\n",
    "\n",
    "def maml_loss(p, x1, y1, x2, y2):\n",
    "    p2 = inner_update(p, x1, y1)\n",
    "    return loss(p2, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tasks(outer_batch_size, inner_batch_size):\n",
    "    # Select amplitude and phase for the task\n",
    "    As = []\n",
    "    phases = []\n",
    "    for _ in range(outer_batch_size):        \n",
    "        As.append(onp.random.uniform(low=0.1, high=.5))\n",
    "        phases.append(onp.random.uniform(low=0., high=np.pi))\n",
    "    def get_batch():\n",
    "        xs, ys = [], []\n",
    "        for A, phase in zip(As, phases):\n",
    "            x = onp.random.uniform(low=-5., high=5., size=(inner_batch_size, 1))\n",
    "            y = A * onp.sin(x + phase)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.stack(xs), np.stack(ys)\n",
    "    x1, y1 = get_batch()\n",
    "    x2, y2 = get_batch()\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update, get_params = optimizers.adam(step_size=1e-3)\n",
    "out_shape, net_params = net_init(rng, in_shape)\n",
    "opt_state = opt_init(net_params)\n",
    "\n",
    "# vmapped version of maml loss.\n",
    "# returns scalar for all tasks.\n",
    "def batch_maml_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    task_losses = vmap(partial(maml_loss, p))(x1_b, y1_b, x2_b, y2_b)\n",
    "    return np.mean(task_losses)\n",
    "\n",
    "@jit\n",
    "def step(i, opt_state, x1, y1, x2, y2):\n",
    "    p = get_params(opt_state)\n",
    "    g = grad(batch_maml_loss)(p, x1, y1, x2, y2)\n",
    "    l = batch_maml_loss(p, x1, y1, x2, y2)\n",
    "    return opt_update(i, g, opt_state), l\n",
    "\n",
    "np_batched_maml_loss = []\n",
    "K=20\n",
    "for i in range(20000):\n",
    "    x1_b, y1_b, x2_b, y2_b = sample_tasks(4, K)\n",
    "    opt_state, l = step(i, opt_state, x1_b, y1_b, x2_b, y2_b)\n",
    "    np_batched_maml_loss.append(l)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "net_params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch the inference across K=100\n",
    "xrange_inputs = np.linspace(-5,5,100).reshape((100, 1))\n",
    "targets = np.sin(xrange_inputs)\n",
    "predictions = vmap(partial(net_apply, net_params))(xrange_inputs)\n",
    "plt.plot(xrange_inputs, predictions, label='pre-update predictions')\n",
    "plt.plot(xrange_inputs, targets, label='target')\n",
    "\n",
    "x1 = onp.random.uniform(low=-5., high=5., size=(10,1))\n",
    "y1 = 1. * onp.sin(x1 + 0.)\n",
    "\n",
    "for i in range(1,3):\n",
    "    net_params = inner_update(net_params, x1, y1)\n",
    "    predictions = vmap(partial(net_apply, net_params))(xrange_inputs)\n",
    "    plt.plot(xrange_inputs, predictions, label='{}-shot predictions'.format(i))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
